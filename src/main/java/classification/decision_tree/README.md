# 决策树算法
## 一、决策树种类分类：

决策树分为两类：分类树和回归树。

分类树用于分类标签值，输出是定性的，如ID3,C4.5算法

回归树用于预测实际值，输出是定量的，比如温度，年龄等

## 二、构造决策树的过程：
+ **决策树的特征选择**

    对分类有影响的因素
+ **决策树的生成（ID3,C4.5...）**

    哪一个特征是树的根节点，哪一个特征是树的中间节点
+ **决策树的剪枝**
    
    提高泛化能力，防止过拟合；
    
    避免局部最优
    
## 三、决策树算法分类
+  **ID3:** ID3决策树学习算法以 **“信息增益”** 为准则来划分属性。

“信息熵”是度量样本集合纯度最常用的一种指标。假设当前样本集合D中的第K类样本所占的比例为Pk(k = 1,2,3...|y|),
    
则D的信息熵定义为：
Ent(D) = -∑Pk log2​Pk。
Ent(D)的值越小，则D的纯度越高

​
信息增益 = Ent(前) - Ent(后)

增益率: Gain(D,a)=Ent(D)−Ent(D∣a)

一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。

（**个人体会：** ID3算法中初始选择划分属性时：可以对信息增益设置一个阈值，当计算出来的最优的信息增益也比这个阈值小，
       
   可以不需要再往下划分了，该特征没有划分性，其类别标记为数据集里面最多的标签） 
+ **C4.5:** 不直接使用信息增益，而是使用 **“增益率”** 来选择最优划分属性。

    信息增益准则对可取值数目较多的属性有所偏好，同时增益准则对可取数目较少的属性有所偏好。
    
    因此C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。
+ **CART:** CART使用 **“基尼值数”** 来选择划分属性。在候选属性集合A中，选择那个使得划分后基尼值数最小的属性作为最优的划分属性。

    基尼指数（基尼不纯度），表示在样本集合中一个随机选中的样本被分错的概率。
    基尼指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。
    即 基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。

    书中公式：
    Gini(D) = ∑Pk(1−Pk) = 1−∑P2k

注：CART决策树对于分类和回归任务都可用：

对于分类树、采用基尼指数最小化准则进行特征选择，然后生成一个二叉树。

对于回归树、采用平方误差最小化准则（即最小二乘法）
    
