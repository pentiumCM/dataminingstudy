# 决策树算法
## 一、决策树算法分类
+  **ID3:** ID3决策树学习算法以 **“信息增益”** 为准则来划分属性。

“信息熵”是度量样本集合纯度最常用的一种指标。假设当前样本集合D中的第K类样本所占的比例为Pk(k = 1,2,3...|y|),
    
则D的信息熵定义为：
Ent(D) = -∑Pk log2​Pk。
Ent(D)的值越小，则D的纯度越高

​
信息增益 = Ent(前) - Ent(后)

增益率: Gain(D,a)=Ent(D)−Ent(D∣a)

一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。
    
+ **C4.5:** 不直接使用信息增益，而是使用 **“增益率”** 来选择最优划分属性。

    信息增益准则对可取值数目较多的属性有所偏好，同时增益准则对可取数目较少的属性有所偏好。
    
    因此C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。
+ **CART:** CART使用 **“基尼值数”** 来选择划分属性。在候选属性集合A中，选择那个使得划分后基尼值数最小的属性作为最优的划分属性。

    基尼指数（基尼不纯度），表示在样本集合中一个随机选中的样本被分错的概率。
    基尼指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。
    即 基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率。

    书中公式：
    Gini(D) = ∑Pk(1−Pk) = 1−∑P2k
    
