# 支持向量机SVM
## 一、概念：
+ **SVM基本思想：** 给定训练样本集D = {(x1,y1)(x2,y2).....(xm,ym)}, yi = {-1,+1}。
分类学习最基本的想法就是基于训练集D在样本空间中找到一个划分超平面，将不同类别的样本分来。
应该找位于两类训练样本“正中间”的划分超平面，该划分超平面对训练样本的局部扰动的“容忍”性最好，鲁棒性好，泛化能力最强。

+ **划分超平面:**
    在样本空间中，划分超平面可以通过如下的线性方程来描述：
    
    W(T)X + b = 0, 其中W = (w1;w2;....wd)为法向量，决定了超平面的方向（W垂直于超平面且为d*1的矩阵形式）。b为位移项，决定了超平面与原点的距离。
    
    显然，划分超平面可被法向量和W和位移b确定，所以划分超平面可以记为(W,b)。
    
+ **计算样本点到超平面的距离：**
    样本空间中任意一点X到超平面(W,b)的距离可写为r = |W(T)X + b| / ||W||。

+ **支持向量：**
    距离超平面最近的这几个训练样本点使得等号成立。

+ **间隔：**
    如果超平面能够将训练样本分类，即对于(xi,yi),令(*)式：
    
    W(T)X + b >= +1, yi = +1;
    
    W(T)X + b <= -1, yi = -1.
    
    两个异类支持向量到超平面的距离之和为 r = 2 / ||W||,它被成为“间隔”。

+ **拉格朗日乘子法:**
    欲找到最大具有“间隔”的划分超平面，即找到能满足(*)式中约束的参数W和b，使得r最大，即
    
    max(2 / ||W||)  s.t. yi(W(T)xi + b) >= 1, i = 1,2......m.
    
    显然，为了最大化间隔，仅需最大化 ||W||(-1),这就等价于最小化||W||(2)。于是，上式可重写为
    
    min (1/2)||W||(2)  s.t. yi(W(T)xi + b) >= 1, i = 1,2......m.
    
    这就是支持向量机的基本型。
    
+ **KKT条件：**


+ **核函数：**

1. **应用问题：**  对于这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。
2. **核函数定义：** 令X为输入空间，k(.,.)是定义在 X * X 上的对称函数，
则k是核函数当且仅当对于任意数据 D = {x1,x2....xm},“核矩阵”**K** 总是正定的:

![](https://private.codecogs.com/gif.latex?K%20%3D%20%5Cbegin%7Bbmatrix%7D%20%5Ckappa%20%28x_%7B1%7D%2Cx_%7B1%7D%29%20%26%20.%20%26%20.%20%26%20.%20%26%20%5Ckappa%20%28x_%7B1%7D%2Cx_%7Bm%7D%29%20%5C%5C%20.%20%26%20.%20%26%20.%20%26%20.%20%26%20.%5C%5C%20.%20%26%20.%20%26%20.%20%26%20.%20%26%20.%5C%5C%20.%20%26%20.%20%26%20.%20%26%20.%20%26%20.%5C%5C%20%5Ckappa%20%28x_%7Bm%7D%2Cx_%7B1%7D%29%20%26%20.%20%26%20.%20%26%20.%20%26%20%5Ckappa%20%28x_%7Bm%7D%2Cx_%7Bm%7D%29%20%5Cend%7Bbmatrix%7D)

k(xi,xj) = (g(xi),g(xj)) = g(xi)(T) * g(xj),

注：g(xi)是将样本点xi从样本空间映射到更高维的特征空间中。
